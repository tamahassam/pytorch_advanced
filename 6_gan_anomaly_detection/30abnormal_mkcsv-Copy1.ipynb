{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secret-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bibliographic-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_lyr:\n",
    "    def input_lyr(z_dim, image_size):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, image_size ** 2 // 8, \n",
    "                              kernel_size=4, stride=1),\n",
    "            nn.BatchNorm2d(image_size ** 2 // 8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        return lyr\n",
    "\n",
    "    def hdn_lyr(image_size, i):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.ConvTranspose2d(image_size * (2 ** (i+1)),\n",
    "                               image_size * (2 ** i), \n",
    "                               kernel_size=4, \n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(image_size * (2 ** i)),\n",
    "            nn.ReLU(inplace=True))\n",
    "        return lyr\n",
    "\n",
    "    def output_lyr(image_size):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.ConvTranspose2d(image_size, 1, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "                nn.Tanh())\n",
    "        return lyr\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_lyr = gen_lyr.input_lyr(z_dim, image_size)\n",
    "        self.hdn_lyr2 = gen_lyr.hdn_lyr(image_size, 2)\n",
    "        self.hdn_lyr1 = gen_lyr.hdn_lyr(image_size, 1)\n",
    "        self.hdn_lyr0 = gen_lyr.hdn_lyr(image_size, 0)\n",
    "        self.output_lyr = gen_lyr.output_lyr(image_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.input_lyr(z)\n",
    "        out = self.hdn_lyr2(out)\n",
    "        out = self.hdn_lyr1(out)\n",
    "        out = self.hdn_lyr0(out)\n",
    "        out = self.output_lyr(out)\n",
    "\n",
    "        return out\n",
    "class dis_lyr:\n",
    "    def input_lyr(z_dim, image_size):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.Conv2d(1, image_size, kernel_size=4,\n",
    "                     stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        return lyr\n",
    "    \n",
    "    def hdn_lyr(image_size, i):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.Conv2d(image_size * (2 ** i),\n",
    "                      image_size * (2 ** (i+1)),\n",
    "                      kernel_size=4,\n",
    "                      stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        return lyr\n",
    "    \n",
    "    def output_lyr(image_size):\n",
    "        lyr = nn.Sequential(\n",
    "            nn.Conv2d(image_size ** 2 // 8, 1,\n",
    "                      kernel_size=4, stride=1))\n",
    "        return lyr\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super().__init__()\n",
    "        self.input_lyr = dis_lyr.input_lyr(z_dim, image_size)\n",
    "        self.hdn_lyr0 = dis_lyr.hdn_lyr(image_size, 0)\n",
    "        self.hdn_lyr1 = dis_lyr.hdn_lyr(image_size, 1)\n",
    "        self.hdn_lyr2 = dis_lyr.hdn_lyr(image_size, 2)\n",
    "        self.output_lyr = dis_lyr.output_lyr(image_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.input_lyr(x)\n",
    "        out = self.hdn_lyr0(out)\n",
    "        out = self.hdn_lyr1(out)\n",
    "        out = self.hdn_lyr2(out)\n",
    "\n",
    "        feature = out  # 最後にチャネルを1つに集約する手前の情報\n",
    "        feature = feature.view(feature.size()[0], -1)  # 2次元に変換\n",
    "\n",
    "        out = self.output_lyr(out)\n",
    "\n",
    "        return out, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (input_lyr): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (hdn_lyr0): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (hdn_lyr1): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (hdn_lyr2): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (output_lyr): Sequential(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(z_dim=10, image_size=64)\n",
    "D = Discriminator(z_dim=10, image_size=64)\n",
    "G.to(device)\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "israeli-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するモデル\n",
    "G_load_path = \"/mnt/home/atsuk/Desktop/pytorch_advanced/6_gan_anomaly_detection/G_ano_dc_64_10_30ep.pth\"\n",
    "D_load_path = \"/mnt/home/atsuk/Desktop/pytorch_advanced/6_gan_anomaly_detection/D_ano_dc_64_10_30ep.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funded-lodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU上で保存された重みをCPU上でロードする場合\n",
    "G_load_weights = torch.load(G_load_path, map_location={'cuda:0': 'cpu'})\n",
    "G.load_state_dict(G_load_weights)\n",
    "D_load_weights = torch.load(D_load_path, map_location={'cuda:0': 'cpu'})\n",
    "D.load_state_dict(D_load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indonesian-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Anomaly_score(x, fake_img, D, Lambda=0.1):\n",
    "\n",
    "    # テスト画像xと生成画像fake_imgのピクセルレベルの差の絶対値を求めて、ミニバッチごとに和を求める\n",
    "    residual_loss = torch.abs(x-fake_img)\n",
    "    residual_loss = residual_loss.view(residual_loss.size()[0], -1)\n",
    "    residual_loss = torch.sum(residual_loss, dim=1)\n",
    "\n",
    "    # テスト画像xと生成画像fake_imgを識別器Dに入力し、特徴量を取り出す\n",
    "    _, x_feature = D(x)\n",
    "    _, G_feature = D(fake_img)\n",
    "\n",
    "    # テスト画像xと生成画像fake_imgの特徴量の差の絶対値を求めて、ミニバッチごとに和を求める\n",
    "    discrimination_loss = torch.abs(x_feature-G_feature)\n",
    "    discrimination_loss = discrimination_loss.view(\n",
    "        discrimination_loss.size()[0], -1)\n",
    "    discrimination_loss = torch.sum(discrimination_loss, dim=1)\n",
    "\n",
    "    # ミニバッチごとに2種類の損失を足し算する\n",
    "    loss_each = (1-Lambda)*residual_loss + Lambda*discrimination_loss\n",
    "\n",
    "    # ミニバッチ全部の損失を求める\n",
    "    total_loss = torch.sum(loss_each)\n",
    "\n",
    "    return total_loss, loss_each, residual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adolescent-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(path, i):\n",
    "    \"\"\"学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する。 \"\"\"\n",
    "    til = glob.glob(path)\n",
    "    random.shuffle(til)\n",
    "    if i == \"a\":\n",
    "        train_img_list = til[:]\n",
    "    else:\n",
    "        train_img_list = til[:i]\n",
    "    return train_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acceptable-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    \"\"\"画像の前処理クラス\"\"\"\n",
    "\n",
    "    def __init__(self, resize):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(resize),  # リサイズ\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "applied-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Img_Dataset(data.Dataset):\n",
    "    \"\"\"画像のDatasetクラス。PyTorchのDatasetクラスを継承\"\"\"\n",
    "\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        '''画像の枚数を返す'''\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''前処理をした画像のTensor形式のデータを取得'''\n",
    "\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)  # [高さ][幅]白黒\n",
    "\n",
    "        # 画像の前処理\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        return img_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alike-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成と動作確認\n",
    "\n",
    "# ファイルリストを作成\n",
    "num_of_data = \"a\"\n",
    "# path = '/media/tamahassam/Extreme SSD/normal_ct_visual/*.tif'\n",
    "path = '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/*.png'\n",
    "test_img_list=make_datapath_list(path, num_of_data)\n",
    "random.shuffle(test_img_list)\n",
    "\n",
    "# 異常のDataLoaderの作成\n",
    "\n",
    "# Datasetを作成\n",
    "# mean = (0.5,)\n",
    "# std = (0.5,)\n",
    "resize = 64\n",
    "test_dataset = GAN_Img_Dataset(\n",
    "    file_list= test_img_list, transform=ImageTransform(resize))\n",
    "\n",
    "# DataLoaderを作成\n",
    "batch_size = 1\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latest-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_59.png',\n",
       " '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_9118.png',\n",
       " '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_8374.png',\n",
       " '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_15403.png',\n",
       " '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_14795.png']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "given-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/mnt/home/atsuk/Desktop/abnormal_detection_data/abnormal_path_30_a.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(test_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "living-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_59.png', '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_9118.png', '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_8374.png', '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_15403.png', '/mnt/home/atsuk/Desktop/abnormal_detection_data/covid_ct_good3_final/covid_14795.png']\n",
      "11008\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "with open('/mnt/home/atsuk/Desktop/abnormal_detection_data/abnormal_path_30_a.csv', 'r') as csv_file:\n",
    "    csv_reader = reader(csv_file)\n",
    "    # Passing the cav_reader object to list() to get a list of lists\n",
    "    list_of_rows = list(csv_reader)\n",
    "    print(list_of_rows[0][:5])\n",
    "    print(len(list_of_rows[0]))\n",
    "    # print(type(csv_reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-cabin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/11008 [03:24<624:31:11, 204.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 690\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in tqdm(range(len(test_img_list))):\n",
    "# for i in tqdm(range(2500)):\n",
    "    # 異常検知したい画像\n",
    "    # テストデータの確認\n",
    "    batch_iterator = iter(test_dataloader)  # イテレータに変換\n",
    "    imges = next(batch_iterator)  \n",
    "\n",
    "    x = imges[[0]]\n",
    "    x = x.to(device)\n",
    "\n",
    "    # 異常検知したい画像を生成するための、初期乱数\n",
    "    z = torch.randn(1, 10).to(device)\n",
    "    z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "\n",
    "    # 変数zを微分を求めることが可能なように、requires_gradをTrueに設定\n",
    "    z.requires_grad = True\n",
    "\n",
    "    # 変数zを更新できるように、zの最適化関数を求める\n",
    "    z_optimizer = torch.optim.Adam([z], lr=1e-3)\n",
    "\n",
    "    # zを求める\n",
    "    for epoch in range(5000+1):\n",
    "        fake_img = G(z)\n",
    "        loss, _, _ = Anomaly_score(x, fake_img, D, Lambda=0.1)\n",
    "\n",
    "        z_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "        # if epoch % 1000 == 0:\n",
    "        #     print('epoch {} || loss_total:{:.0f} '.format(epoch, loss.item()))\n",
    "        if epoch == 5000:\n",
    "            print(f'loss: {round(loss.item())}')\n",
    "            losses.append(round(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/mnt/home/atsuk/Desktop/abnormal_detection_data/30_abnormal_losses_a.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-suspension",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
